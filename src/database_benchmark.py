import time
import psutil
import os
import json
from typing import Dict, List
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter
from datetime import datetime
import lancedb
import chromadb
from qdrant_client import QdrantClient
from pymilvus import MilvusClient
import weaviate
import psycopg2
from sentence_transformers import SentenceTransformer

class DatabaseBenchmark:
    def __init__(self):
        self.results = {
            "lancedb": {},
            "chromadb": {},
            "qdrant": {},
            "milvus": {},
            "weaviate": {},
            "pgvector": {}
        }
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        
        # Test sorgularÄ±
        self.test_queries = [
            "artificial intelligence healthcare",
            "machine learning medical diagnosis",
            "deep learning neural networks",
            "natural language processing",
            "computer vision medical imaging"
        ]
        
    def measure_memory(self) -> float:
        """Bellek kullanÄ±mÄ±nÄ± MB cinsinden Ã¶lÃ§"""
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    
    def measure_disk_size(self, path: str) -> float:
        """KlasÃ¶r boyutunu MB cinsinden Ã¶lÃ§"""
        if not os.path.exists(path):
            return 0
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(filepath)
                except:
                    pass
        return total_size / 1024 / 1024
    
    # ==================== LanceDB Benchmark ====================
    def benchmark_lancedb(self):
        """LanceDB performansÄ±nÄ± Ã¶lÃ§"""
        print("\n" + "="*60)
        print("ğŸ“Š LanceDB BENCHMARK")
        print("="*60)
        
        try:
            db_path = "/home/ugo/Documents/Python/bitirememe projesi/DB/lanceDatabase/db"
            
            if not os.path.exists(db_path):
                print(f"âš  LanceDB veritabanÄ± bulunamadÄ±: {db_path}")
                self.results["lancedb"]["error"] = "Database not found"
                return
            
            # BaÄŸlantÄ± zamanÄ±
            start = time.time()
            db = lancedb.connect(db_path)
            connection_time = time.time() - start
            self.results["lancedb"]["connection_time"] = connection_time
            print(f"âœ“ BaÄŸlantÄ± zamanÄ±: {connection_time:.4f}s")
            
            # Tablo aÃ§ma zamanÄ±
            start = time.time()
            table = db.open_table("documents")
            open_time = time.time() - start
            self.results["lancedb"]["open_table_time"] = open_time
            print(f"âœ“ Tablo aÃ§ma zamanÄ±: {open_time:.4f}s")
            
            # KayÄ±t sayÄ±sÄ±
            start = time.time()
            count = len(table.to_pandas())
            count_time = time.time() - start
            self.results["lancedb"]["record_count"] = count
            self.results["lancedb"]["count_time"] = count_time
            print(f"âœ“ Toplam kayÄ±t: {count} ({count_time:.4f}s)")
            
            # Arama performansÄ± (soÄŸuk baÅŸlangÄ±Ã§)
            print("\nğŸ” SoÄŸuk baÅŸlangÄ±Ã§ aramalarÄ±...")
            cold_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                start = time.time()
                results = table.search(query).limit(5).to_pandas()
                search_time = time.time() - start
                cold_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_cold_search = sum(cold_search_times) / len(cold_search_times)
            self.results["lancedb"]["avg_cold_search_time"] = avg_cold_search
            self.results["lancedb"]["min_cold_search_time"] = min(cold_search_times)
            self.results["lancedb"]["max_cold_search_time"] = max(cold_search_times)
            print(f"  Ortalama: {avg_cold_search:.4f}s")
            
            # Arama performansÄ± (sÄ±cak - Ã¶nbellekli)
            print("\nğŸ”¥ SÄ±cak (cached) aramalar...")
            hot_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                start = time.time()
                results = table.search(query).limit(5).to_pandas()
                search_time = time.time() - start
                hot_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_hot_search = sum(hot_search_times) / len(hot_search_times)
            self.results["lancedb"]["avg_hot_search_time"] = avg_hot_search
            self.results["lancedb"]["min_hot_search_time"] = min(hot_search_times)
            self.results["lancedb"]["max_hot_search_time"] = max(hot_search_times)
            print(f"  Ortalama: {avg_hot_search:.4f}s")
            
            # Disk boyutu
            disk_size = self.measure_disk_size(db_path)
            self.results["lancedb"]["disk_size_mb"] = disk_size
            print(f"\nğŸ’¾ Disk boyutu: {disk_size:.2f} MB")
            
            # Bellek kullanÄ±mÄ±
            start_mem = self.measure_memory()
            _ = table.to_pandas()
            end_mem = self.measure_memory()
            memory_used = end_mem - start_mem
            self.results["lancedb"]["memory_used_mb"] = memory_used
            print(f"ğŸ§  Bellek kullanÄ±mÄ±: {memory_used:.2f} MB")
            
            print("âœ… LanceDB benchmark tamamlandÄ±")
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            self.results["lancedb"]["error"] = str(e)
    
    # ==================== ChromaDB Benchmark ====================
    def benchmark_chromadb(self):
        """ChromaDB performansÄ±nÄ± Ã¶lÃ§"""
        print("\n" + "="*60)
        print("ğŸ“Š ChromaDB BENCHMARK")
        print("="*60)
        
        try:
            db_path = "/home/ugo/Documents/Python/bitirememe projesi/DB/chorame/yerel_veritabani"
            
            if not os.path.exists(db_path):
                print(f"âš  ChromaDB veritabanÄ± bulunamadÄ±: {db_path}")
                self.results["chromadb"]["error"] = "Database not found"
                return
            
            # BaÄŸlantÄ± zamanÄ±
            start = time.time()
            client = chromadb.PersistentClient(path=db_path)
            connection_time = time.time() - start
            self.results["chromadb"]["connection_time"] = connection_time
            print(f"âœ“ BaÄŸlantÄ± zamanÄ±: {connection_time:.4f}s")
            
            # Collection aÃ§ma
            start = time.time()
            collection = client.get_or_create_collection(name="dokumanlarim")
            collection_time = time.time() - start
            self.results["chromadb"]["collection_open_time"] = collection_time
            print(f"âœ“ Collection aÃ§ma zamanÄ±: {collection_time:.4f}s")
            
            # KayÄ±t sayÄ±sÄ±
            start = time.time()
            count = collection.count()
            count_time = time.time() - start
            self.results["chromadb"]["record_count"] = count
            self.results["chromadb"]["count_time"] = count_time
            print(f"âœ“ Toplam kayÄ±t: {count} ({count_time:.4f}s)")
            
            # SoÄŸuk arama
            print("\nğŸ” SoÄŸuk baÅŸlangÄ±Ã§ aramalarÄ±...")
            cold_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                start = time.time()
                results = collection.query(query_texts=[query], n_results=5)
                search_time = time.time() - start
                cold_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_cold_search = sum(cold_search_times) / len(cold_search_times)
            self.results["chromadb"]["avg_cold_search_time"] = avg_cold_search
            self.results["chromadb"]["min_cold_search_time"] = min(cold_search_times)
            self.results["chromadb"]["max_cold_search_time"] = max(cold_search_times)
            print(f"  Ortalama: {avg_cold_search:.4f}s")
            
            # SÄ±cak arama
            print("\nğŸ”¥ SÄ±cak (cached) aramalar...")
            hot_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                start = time.time()
                results = collection.query(query_texts=[query], n_results=5)
                search_time = time.time() - start
                hot_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_hot_search = sum(hot_search_times) / len(hot_search_times)
            self.results["chromadb"]["avg_hot_search_time"] = avg_hot_search
            self.results["chromadb"]["min_hot_search_time"] = min(hot_search_times)
            self.results["chromadb"]["max_hot_search_time"] = max(hot_search_times)
            print(f"  Ortalama: {avg_hot_search:.4f}s")
            
            # Disk boyutu
            disk_size = self.measure_disk_size(db_path)
            self.results["chromadb"]["disk_size_mb"] = disk_size
            print(f"\nğŸ’¾ Disk boyutu: {disk_size:.2f} MB")
            
            print("âœ… ChromaDB benchmark tamamlandÄ±")
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            self.results["chromadb"]["error"] = str(e)
    
    # ==================== Qdrant Benchmark ====================
    def benchmark_qdrant(self):
        """Qdrant performansÄ±nÄ± Ã¶lÃ§"""
        print("\n" + "="*60)
        print("ğŸ“Š Qdrant BENCHMARK")
        print("="*60)
        
        try:
            # BaÄŸlantÄ± zamanÄ±
            start = time.time()
            client = QdrantClient(host="localhost", port=6333)
            connection_time = time.time() - start
            self.results["qdrant"]["connection_time"] = connection_time
            print(f"âœ“ BaÄŸlantÄ± zamanÄ±: {connection_time:.4f}s")
            
            # Collection bilgisi
            try:
                start = time.time()
                collection_info = client.get_collection("test_collection")
                info_time = time.time() - start
                self.results["qdrant"]["info_time"] = info_time
                print(f"âœ“ Collection info zamanÄ±: {info_time:.4f}s")
                
                # KayÄ±t sayÄ±sÄ±
                count = collection_info.points_count
                self.results["qdrant"]["record_count"] = count
                print(f"âœ“ Toplam kayÄ±t: {count}")
                
                # SoÄŸuk arama
                print("\nğŸ” SoÄŸuk baÅŸlangÄ±Ã§ aramalarÄ±...")
                cold_search_times = []
                for i, query in enumerate(self.test_queries, 1):
                    query_vector = self.model.encode(query).tolist()
                    start = time.time()
                    results = client.query_points(
                        collection_name="test_collection",
                        query=query_vector,
                        limit=5,
                        with_payload=True
                    )
                    search_time = time.time() - start
                    cold_search_times.append(search_time)
                    print(f"  Sorgu {i}: {search_time:.4f}s")
                
                avg_cold_search = sum(cold_search_times) / len(cold_search_times)
                self.results["qdrant"]["avg_cold_search_time"] = avg_cold_search
                self.results["qdrant"]["min_cold_search_time"] = min(cold_search_times)
                self.results["qdrant"]["max_cold_search_time"] = max(cold_search_times)
                print(f"  Ortalama: {avg_cold_search:.4f}s")
                
                # SÄ±cak arama
                print("\nğŸ”¥ SÄ±cak (cached) aramalar...")
                hot_search_times = []
                for i, query in enumerate(self.test_queries, 1):
                    query_vector = self.model.encode(query).tolist()
                    start = time.time()
                    results = client.query_points(
                        collection_name="test_collection",
                        query=query_vector,
                        limit=5,
                        with_payload=True
                    )
                    search_time = time.time() - start
                    hot_search_times.append(search_time)
                    print(f"  Sorgu {i}: {search_time:.4f}s")
                
                avg_hot_search = sum(hot_search_times) / len(hot_search_times)
                self.results["qdrant"]["avg_hot_search_time"] = avg_hot_search
                self.results["qdrant"]["min_hot_search_time"] = min(hot_search_times)
                self.results["qdrant"]["max_hot_search_time"] = max(hot_search_times)
                print(f"  Ortalama: {avg_hot_search:.4f}s")
                
                print("âœ… Qdrant benchmark tamamlandÄ±")
                
            except Exception as e:
                print(f"âš  Qdrant collection bulunamadÄ± veya boÅŸ: {e}")
                self.results["qdrant"]["error"] = "Collection not found or empty"
            
        except Exception as e:
            print(f"âŒ Qdrant baÄŸlantÄ± hatasÄ± (sunucu Ã§alÄ±ÅŸmÄ±yor olabilir): {e}")
            self.results["qdrant"]["error"] = f"Connection failed: {str(e)}"
    
    # ==================== Milvus Benchmark ====================
    def benchmark_milvus(self):
        """Milvus performansÄ±nÄ± Ã¶lÃ§"""
        print("\n" + "="*60)
        print("ğŸ“Š Milvus BENCHMARK")
        print("="*60)
        
        try:
            db_path = "/home/ugo/Documents/Python/bitirememe projesi/DB/milvus/milvus_demo.db"
            
            if not os.path.exists(db_path):
                print(f"âš  Milvus veritabanÄ± bulunamadÄ±: {db_path}")
                self.results["milvus"]["error"] = "Database not found"
                return
            
            # BaÄŸlantÄ± zamanÄ±
            start = time.time()
            client = MilvusClient(db_path)
            connection_time = time.time() - start
            self.results["milvus"]["connection_time"] = connection_time
            print(f"âœ“ BaÄŸlantÄ± zamanÄ±: {connection_time:.4f}s")
            
            # KayÄ±t sayÄ±sÄ±
            start = time.time()
            stats = client.get_collection_stats(collection_name="documents")
            count = stats['row_count']
            count_time = time.time() - start
            self.results["milvus"]["record_count"] = count
            self.results["milvus"]["count_time"] = count_time
            print(f"âœ“ Toplam kayÄ±t: {count} ({count_time:.4f}s)")
            
            # SoÄŸuk arama
            print("\nğŸ” SoÄŸuk baÅŸlangÄ±Ã§ aramalarÄ±...")
            cold_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                query_vector = self.model.encode(query).tolist()
                start = time.time()
                results = client.search(
                    collection_name="documents",
                    data=[query_vector],
                    limit=5,
                    output_fields=["metin", "chunk_id", "doc_id"]
                )
                search_time = time.time() - start
                cold_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_cold_search = sum(cold_search_times) / len(cold_search_times)
            self.results["milvus"]["avg_cold_search_time"] = avg_cold_search
            self.results["milvus"]["min_cold_search_time"] = min(cold_search_times)
            self.results["milvus"]["max_cold_search_time"] = max(cold_search_times)
            print(f"  Ortalama: {avg_cold_search:.4f}s")
            
            # SÄ±cak arama
            print("\nğŸ”¥ SÄ±cak (cached) aramalar...")
            hot_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                query_vector = self.model.encode(query).tolist()
                start = time.time()
                results = client.search(
                    collection_name="documents",
                    data=[query_vector],
                    limit=5,
                    output_fields=["metin", "chunk_id", "doc_id"]
                )
                search_time = time.time() - start
                hot_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_hot_search = sum(hot_search_times) / len(hot_search_times)
            self.results["milvus"]["avg_hot_search_time"] = avg_hot_search
            self.results["milvus"]["min_hot_search_time"] = min(hot_search_times)
            self.results["milvus"]["max_hot_search_time"] = max(hot_search_times)
            print(f"  Ortalama: {avg_hot_search:.4f}s")
            
            # Disk boyutu
            disk_size = os.path.getsize(db_path) / 1024 / 1024
            self.results["milvus"]["disk_size_mb"] = disk_size
            print(f"\nğŸ’¾ Disk boyutu: {disk_size:.2f} MB")
            
            print("âœ… Milvus benchmark tamamlandÄ±")
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            self.results["milvus"]["error"] = str(e)
    
    # ==================== Weaviate Benchmark ====================
    def benchmark_weaviate(self):
        """Weaviate performansÄ±nÄ± Ã¶lÃ§"""
        print("\n" + "="*60)
        print("ğŸ“Š Weaviate BENCHMARK")
        print("="*60)
        
        try:
            # BaÄŸlantÄ± zamanÄ±
            start = time.time()
            client = weaviate.connect_to_local()
            connection_time = time.time() - start
            self.results["weaviate"]["connection_time"] = connection_time
            print(f"âœ“ BaÄŸlantÄ± zamanÄ±: {connection_time:.4f}s")
            
            try:
                collection = client.collections.get("Documents")
                
                # KayÄ±t sayÄ±sÄ±
                start = time.time()
                agg = collection.aggregate.over_all(total_count=True)
                count = agg.total_count
                count_time = time.time() - start
                self.results["weaviate"]["record_count"] = count
                self.results["weaviate"]["count_time"] = count_time
                print(f"âœ“ Toplam kayÄ±t: {count} ({count_time:.4f}s)")
                
                # SoÄŸuk arama (Vector)
                print("\nğŸ” SoÄŸuk baÅŸlangÄ±Ã§ aramalarÄ± (Vector)...")
                cold_search_times = []
                for i, query in enumerate(self.test_queries, 1):
                    query_vector = self.model.encode(query).tolist()
                    start = time.time()
                    results = collection.query.near_vector(
                        near_vector=query_vector,
                        limit=5
                    )
                    search_time = time.time() - start
                    cold_search_times.append(search_time)
                    print(f"  Sorgu {i}: {search_time:.4f}s")
                
                avg_cold_search = sum(cold_search_times) / len(cold_search_times)
                self.results["weaviate"]["avg_cold_search_time"] = avg_cold_search
                self.results["weaviate"]["min_cold_search_time"] = min(cold_search_times)
                self.results["weaviate"]["max_cold_search_time"] = max(cold_search_times)
                print(f"  Ortalama: {avg_cold_search:.4f}s")
                
                # SÄ±cak arama (Vector)
                print("\nğŸ”¥ SÄ±cak aramalar (Vector)...")
                hot_search_times = []
                for i, query in enumerate(self.test_queries, 1):
                    query_vector = self.model.encode(query).tolist()
                    start = time.time()
                    results = collection.query.near_vector(
                        near_vector=query_vector,
                        limit=5
                    )
                    search_time = time.time() - start
                    hot_search_times.append(search_time)
                    print(f"  Sorgu {i}: {search_time:.4f}s")
                
                avg_hot_search = sum(hot_search_times) / len(hot_search_times)
                self.results["weaviate"]["avg_hot_search_time"] = avg_hot_search
                self.results["weaviate"]["min_hot_search_time"] = min(hot_search_times)
                self.results["weaviate"]["max_hot_search_time"] = max(hot_search_times)
                print(f"  Ortalama: {avg_hot_search:.4f}s")
                
                # Hybrid arama
                print("\nğŸ”¥ Hybrid arama (Vector + BM25)...")
                hybrid_search_times = []
                for i, query in enumerate(self.test_queries, 1):
                    query_vector = self.model.encode(query).tolist()
                    start = time.time()
                    results = collection.query.hybrid(
                        query=query,
                        vector=query_vector,
                        limit=5
                    )
                    search_time = time.time() - start
                    hybrid_search_times.append(search_time)
                    print(f"  Sorgu {i}: {search_time:.4f}s")
                
                avg_hybrid_search = sum(hybrid_search_times) / len(hybrid_search_times)
                self.results["weaviate"]["avg_hybrid_search_time"] = avg_hybrid_search
                print(f"  Ortalama: {avg_hybrid_search:.4f}s")
                
                print("âœ… Weaviate benchmark tamamlandÄ±")
                
            except Exception as e:
                print(f"âš  Weaviate collection bulunamadÄ±: {e}")
                self.results["weaviate"]["error"] = "Collection not found"
            
            client.close()
            
        except Exception as e:
            print(f"âŒ Weaviate baÄŸlantÄ± hatasÄ± (sunucu Ã§alÄ±ÅŸmÄ±yor olabilir): {e}")
            self.results["weaviate"]["error"] = f"Connection failed: {str(e)}"
    
    # ==================== pgvector Benchmark ====================
    def benchmark_pgvector(self):
        """pgvector performansÄ±nÄ± Ã¶lÃ§"""
        print("\n" + "="*60)
        print("ğŸ“Š pgvector (PostgreSQL) BENCHMARK")
        print("="*60)
        
        try:
            # BaÄŸlantÄ± zamanÄ±
            start = time.time()
            conn = psycopg2.connect(
                host="localhost",
                database="vector_db",
                user="postgres",
                password="yeni_sifre",
                port="5432"
            )
            connection_time = time.time() - start
            self.results["pgvector"]["connection_time"] = connection_time
            print(f"âœ“ BaÄŸlantÄ± zamanÄ±: {connection_time:.4f}s")
            
            cursor = conn.cursor()
            
            # KayÄ±t sayÄ±sÄ±
            start = time.time()
            cursor.execute("SELECT COUNT(*) FROM documents;")
            count = cursor.fetchone()[0]
            count_time = time.time() - start
            self.results["pgvector"]["record_count"] = count
            self.results["pgvector"]["count_time"] = count_time
            print(f"âœ“ Toplam kayÄ±t: {count} ({count_time:.4f}s)")
            
            # SoÄŸuk arama (Cosine)
            print("\nğŸ” SoÄŸuk baÅŸlangÄ±Ã§ aramalarÄ± (Cosine)...")
            cold_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                query_vector = self.model.encode(query).tolist()
                embedding_str = '[' + ','.join(map(str, query_vector)) + ']'
                
                start = time.time()
                cursor.execute(f"""
                    SELECT id, chunk_id, metin, 
                           embedding <=> '{embedding_str}'::vector AS distance
                    FROM documents
                    ORDER BY embedding <=> '{embedding_str}'::vector
                    LIMIT 5;
                """)
                results = cursor.fetchall()
                search_time = time.time() - start
                cold_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_cold_search = sum(cold_search_times) / len(cold_search_times)
            self.results["pgvector"]["avg_cold_search_time"] = avg_cold_search
            self.results["pgvector"]["min_cold_search_time"] = min(cold_search_times)
            self.results["pgvector"]["max_cold_search_time"] = max(cold_search_times)
            print(f"  Ortalama: {avg_cold_search:.4f}s")
            
            # SÄ±cak arama (Cosine)
            print("\nğŸ”¥ SÄ±cak aramalar (Cosine)...")
            hot_search_times = []
            for i, query in enumerate(self.test_queries, 1):
                query_vector = self.model.encode(query).tolist()
                embedding_str = '[' + ','.join(map(str, query_vector)) + ']'
                
                start = time.time()
                cursor.execute(f"""
                    SELECT id, chunk_id, metin, 
                           embedding <=> '{embedding_str}'::vector AS distance
                    FROM documents
                    ORDER BY embedding <=> '{embedding_str}'::vector
                    LIMIT 5;
                """)
                results = cursor.fetchall()
                search_time = time.time() - start
                hot_search_times.append(search_time)
                print(f"  Sorgu {i}: {search_time:.4f}s")
            
            avg_hot_search = sum(hot_search_times) / len(hot_search_times)
            self.results["pgvector"]["avg_hot_search_time"] = avg_hot_search
            self.results["pgvector"]["min_hot_search_time"] = min(hot_search_times)
            self.results["pgvector"]["max_hot_search_time"] = max(hot_search_times)
            print(f"  Ortalama: {avg_hot_search:.4f}s")
            
            cursor.close()
            conn.close()
            
            print("âœ… pgvector benchmark tamamlandÄ±")
            
        except Exception as e:
            print(f"âŒ pgvector baÄŸlantÄ± hatasÄ±: {e}")
            self.results["pgvector"]["error"] = f"Connection failed: {str(e)}"
    
    def run_all_benchmarks(self):
        """TÃ¼m benchmark'leri Ã§alÄ±ÅŸtÄ±r"""
        print("\n" + "ğŸš€"*30)
        print("VERITABANI PERFORMANS BENCHMARK'Ä° BAÅLIYOR")
        print("ğŸš€"*30)
        
        self.benchmark_lancedb()
        self.benchmark_chromadb()
        self.benchmark_milvus()
        self.benchmark_weaviate()
        self.benchmark_pgvector()
        self.benchmark_qdrant()
        
        self.print_detailed_comparison()
        self.save_results()
    
    def print_detailed_comparison(self):
        """DetaylÄ± karÅŸÄ±laÅŸtÄ±rmalÄ± sonuÃ§lar"""
        print("\n" + "="*60)
        print("ğŸ“Š DETAYLI KARÅILAÅTIRMA")
        print("="*60)
        
        # BaÄŸlantÄ± ZamanÄ±
        print("\nâš¡ BaÄŸlantÄ± ZamanÄ±:")
        connection_times = []
        for db_name, metrics in self.results.items():
            if "connection_time" in metrics and "error" not in metrics:
                time_val = metrics['connection_time']
                connection_times.append((db_name, time_val))
        
        connection_times.sort(key=lambda x: x[1])
        for i, (db_name, time_val) in enumerate(connection_times, 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            print(f"{emoji} {i}. {db_name:12} : {time_val:.4f}s")
        
        # SoÄŸuk Arama ZamanÄ±
        print("\nğŸ” Ortalama SoÄŸuk Arama ZamanÄ±:")
        cold_search_times = []
        for db_name, metrics in self.results.items():
            if "avg_cold_search_time" in metrics:
                time_val = metrics['avg_cold_search_time']
                cold_search_times.append((db_name, time_val))
        
        cold_search_times.sort(key=lambda x: x[1])
        for i, (db_name, time_val) in enumerate(cold_search_times, 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            print(f"{emoji} {i}. {db_name:12} : {time_val:.4f}s")
        
        # SÄ±cak Arama ZamanÄ±
        print("\nğŸ”¥ Ortalama SÄ±cak Arama ZamanÄ± (Cached):")
        hot_search_times = []
        for db_name, metrics in self.results.items():
            if "avg_hot_search_time" in metrics:
                time_val = metrics['avg_hot_search_time']
                hot_search_times.append((db_name, time_val))
        
        hot_search_times.sort(key=lambda x: x[1])
        for i, (db_name, time_val) in enumerate(hot_search_times, 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            print(f"{emoji} {i}. {db_name:12} : {time_val:.4f}s")
        
        # Disk Boyutu
        print("\nğŸ’¾ Disk Boyutu:")
        disk_sizes = []
        for db_name, metrics in self.results.items():
            if "disk_size_mb" in metrics:
                size_val = metrics['disk_size_mb']
                disk_sizes.append((db_name, size_val))
        
        disk_sizes.sort(key=lambda x: x[1])
        for i, (db_name, size_val) in enumerate(disk_sizes, 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else "  "
            print(f"{emoji} {i}. {db_name:12} : {size_val:.2f} MB")
        
        # KayÄ±t SayÄ±sÄ±
        print("\nğŸ“ˆ KayÄ±t SayÄ±sÄ±:")
        for db_name, metrics in self.results.items():
            if "record_count" in metrics and "error" not in metrics:
                print(f"   {db_name:12} : {metrics['record_count']:,}")
        
        # Hata Durumu
        print("\nâŒ Hata Durumu:")
        errors_found = False
        for db_name, metrics in self.results.items():
            if "error" in metrics:
                print(f"   {db_name:12} : {metrics['error']}")
                errors_found = True
        
        if not errors_found:
            print("   âœ… TÃ¼m veritabanlarÄ± baÅŸarÄ±lÄ±")
        
        # En Ä°yi Performans Ã–zeti
        print("\n" + "="*60)
        print("ğŸ† EN Ä°YÄ° PERFORMANS Ã–ZETÄ°")
        print("="*60)
        
        if connection_times:
            print(f"âš¡ En HÄ±zlÄ± BaÄŸlantÄ±   : {connection_times[0][0]} ({connection_times[0][1]:.4f}s)")
        
        if cold_search_times:
            print(f"ğŸ” En HÄ±zlÄ± SoÄŸuk Arama: {cold_search_times[0][0]} ({cold_search_times[0][1]:.4f}s)")
        
        if hot_search_times:
            print(f"ğŸ”¥ En HÄ±zlÄ± SÄ±cak Arama: {hot_search_times[0][0]} ({hot_search_times[0][1]:.4f}s)")
        
        if disk_sizes:
            print(f"ğŸ’¾ En KÃ¼Ã§Ã¼k Disk       : {disk_sizes[0][0]} ({disk_sizes[0][1]:.2f} MB)")
    
    def save_results(self):
        """SonuÃ§larÄ± Excel dosyasÄ±na kaydet"""
        output_file = "/home/ugo/Documents/Python/bitirememe projesi/dataset_benchmark.xlsx"
        
        # Yeni workbook oluÅŸtur
        wb = openpyxl.Workbook()
        
        # Ã–zet sayfasÄ±
        ws_summary = wb.active
        ws_summary.title = "Ã–zet KarÅŸÄ±laÅŸtÄ±rma"
        
        # Stil tanÄ±mlamalarÄ±
        header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
        subheader_fill = PatternFill(start_color="B4C7E7", end_color="B4C7E7", fill_type="solid")
        header_font = Font(bold=True, color="FFFFFF", size=11)
        title_font = Font(bold=True, size=14)
        center_align = Alignment(horizontal='center', vertical='center')
        border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        # BaÅŸlÄ±k
        ws_summary.merge_cells('A1:H1')
        ws_summary['A1'] = "Vector Database Performance Benchmark"
        ws_summary['A1'].font = Font(bold=True, size=16)
        ws_summary['A1'].alignment = center_align
        
        ws_summary.merge_cells('A2:H2')
        ws_summary['A2'] = f"Tarih: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        ws_summary['A2'].alignment = center_align
        
        # TÃ¼m veritabanlarÄ±nÄ± listele
        db_names = [db for db in self.results.keys() if "error" not in self.results[db]]
        
        # Ana tablo baÅŸlÄ±ÄŸÄ±
        row = 4
        headers = ["VeritabanÄ±", "BaÄŸlantÄ± (s)", "KayÄ±t SayÄ±sÄ±", "SoÄŸuk Arama (s)", "SÄ±cak Arama (s)", "Min Arama (s)", "Max Arama (s)", "Disk (MB)"]
        for col_idx, header in enumerate(headers, 1):
            cell = ws_summary.cell(row=row, column=col_idx)
            cell.value = header
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = center_align
            cell.border = border
        
        # Verileri doldur
        row += 1
        for db_name in db_names:
            metrics = self.results[db_name]
            
            ws_summary.cell(row=row, column=1, value=db_name)
            ws_summary.cell(row=row, column=2, value=round(metrics.get('connection_time', 0), 4))
            ws_summary.cell(row=row, column=3, value=metrics.get('record_count', 0))
            ws_summary.cell(row=row, column=4, value=round(metrics.get('avg_cold_search_time', 0), 4))
            ws_summary.cell(row=row, column=5, value=round(metrics.get('avg_hot_search_time', 0), 4))
            ws_summary.cell(row=row, column=6, value=round(metrics.get('min_hot_search_time', 0), 4))
            ws_summary.cell(row=row, column=7, value=round(metrics.get('max_hot_search_time', 0), 4))
            ws_summary.cell(row=row, column=8, value=round(metrics.get('disk_size_mb', 0), 2))
            
            # Stil uygula
            for col_idx in range(1, 9):
                cell = ws_summary.cell(row=row, column=col_idx)
                cell.border = border
                cell.alignment = center_align
            
            row += 1
        
        # SÄ±ralama tablolarÄ±
        row += 2
        
        # En Ä°yi Performanslar
        ws_summary.merge_cells(f'A{row}:D{row}')
        ws_summary[f'A{row}'] = "ğŸ† EN Ä°YÄ° PERFORMANSLAR"
        ws_summary[f'A{row}'].font = title_font
        ws_summary[f'A{row}'].alignment = center_align
        row += 1
        
        # BaÄŸlantÄ± HÄ±zÄ±
        ws_summary[f'A{row}'] = "SÄ±ra"
        ws_summary[f'B{row}'] = "âš¡ En HÄ±zlÄ± BaÄŸlantÄ±"
        ws_summary[f'C{row}'] = "SÃ¼re (s)"
        ws_summary[f'D{row}'] = "Fark"
        for col in ['A', 'B', 'C', 'D']:
            ws_summary[f'{col}{row}'].fill = subheader_fill
            ws_summary[f'{col}{row}'].font = Font(bold=True)
            ws_summary[f'{col}{row}'].border = border
            ws_summary[f'{col}{row}'].alignment = center_align
        
        row += 1
        connection_times = [(db, m['connection_time']) for db, m in self.results.items() 
                           if 'connection_time' in m and 'error' not in m]
        connection_times.sort(key=lambda x: x[1])
        
        for i, (db_name, time_val) in enumerate(connection_times[:5], 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            ws_summary[f'A{row}'] = emoji
            ws_summary[f'B{row}'] = db_name
            ws_summary[f'C{row}'] = round(time_val, 4)
            if i == 1:
                ws_summary[f'D{row}'] = "En HÄ±zlÄ±"
            else:
                diff_percent = ((time_val / connection_times[0][1]) - 1) * 100
                ws_summary[f'D{row}'] = f"+%{diff_percent:.1f}"
            
            for col in ['A', 'B', 'C', 'D']:
                ws_summary[f'{col}{row}'].border = border
                ws_summary[f'{col}{row}'].alignment = center_align
            row += 1
        
        # SoÄŸuk Arama
        row += 1
        ws_summary[f'A{row}'] = "SÄ±ra"
        ws_summary[f'B{row}'] = "ğŸ” En HÄ±zlÄ± SoÄŸuk Arama"
        ws_summary[f'C{row}'] = "SÃ¼re (s)"
        ws_summary[f'D{row}'] = "Fark"
        for col in ['A', 'B', 'C', 'D']:
            ws_summary[f'{col}{row}'].fill = subheader_fill
            ws_summary[f'{col}{row}'].font = Font(bold=True)
            ws_summary[f'{col}{row}'].border = border
            ws_summary[f'{col}{row}'].alignment = center_align
        
        row += 1
        cold_times = [(db, m['avg_cold_search_time']) for db, m in self.results.items() 
                     if 'avg_cold_search_time' in m]
        cold_times.sort(key=lambda x: x[1])
        
        for i, (db_name, time_val) in enumerate(cold_times[:5], 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            ws_summary[f'A{row}'] = emoji
            ws_summary[f'B{row}'] = db_name
            ws_summary[f'C{row}'] = round(time_val, 4)
            if i == 1:
                ws_summary[f'D{row}'] = "En HÄ±zlÄ±"
            else:
                diff_percent = ((time_val / cold_times[0][1]) - 1) * 100
                ws_summary[f'D{row}'] = f"+%{diff_percent:.1f}"
            
            for col in ['A', 'B', 'C', 'D']:
                ws_summary[f'{col}{row}'].border = border
                ws_summary[f'{col}{row}'].alignment = center_align
            row += 1
        
        # SÄ±cak Arama
        row += 1
        ws_summary[f'A{row}'] = "SÄ±ra"
        ws_summary[f'B{row}'] = "ğŸ”¥ En HÄ±zlÄ± SÄ±cak Arama"
        ws_summary[f'C{row}'] = "SÃ¼re (s)"
        ws_summary[f'D{row}'] = "Fark"
        for col in ['A', 'B', 'C', 'D']:
            ws_summary[f'{col}{row}'].fill = subheader_fill
            ws_summary[f'{col}{row}'].font = Font(bold=True)
            ws_summary[f'{col}{row}'].border = border
            ws_summary[f'{col}{row}'].alignment = center_align
        
        row += 1
        hot_times = [(db, m['avg_hot_search_time']) for db, m in self.results.items() 
                    if 'avg_hot_search_time' in m]
        hot_times.sort(key=lambda x: x[1])
        
        for i, (db_name, time_val) in enumerate(hot_times[:5], 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            ws_summary[f'A{row}'] = emoji
            ws_summary[f'B{row}'] = db_name
            ws_summary[f'C{row}'] = round(time_val, 4)
            if i == 1:
                ws_summary[f'D{row}'] = "En HÄ±zlÄ±"
            else:
                diff_percent = ((time_val / hot_times[0][1]) - 1) * 100
                ws_summary[f'D{row}'] = f"+%{diff_percent:.1f}"
            
            for col in ['A', 'B', 'C', 'D']:
                ws_summary[f'{col}{row}'].border = border
                ws_summary[f'{col}{row}'].alignment = center_align
            row += 1
        
        # Disk Boyutu
        row += 1
        ws_summary[f'A{row}'] = "SÄ±ra"
        ws_summary[f'B{row}'] = "ğŸ’¾ En KÃ¼Ã§Ã¼k Disk KullanÄ±mÄ±"
        ws_summary[f'C{row}'] = "Boyut (MB)"
        ws_summary[f'D{row}'] = "Fark"
        for col in ['A', 'B', 'C', 'D']:
            ws_summary[f'{col}{row}'].fill = subheader_fill
            ws_summary[f'{col}{row}'].font = Font(bold=True)
            ws_summary[f'{col}{row}'].border = border
            ws_summary[f'{col}{row}'].alignment = center_align
        
        row += 1
        disk_sizes = [(db, m['disk_size_mb']) for db, m in self.results.items() 
                     if 'disk_size_mb' in m]
        disk_sizes.sort(key=lambda x: x[1])
        
        for i, (db_name, size_val) in enumerate(disk_sizes[:5], 1):
            emoji = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else f"{i}."
            ws_summary[f'A{row}'] = emoji
            ws_summary[f'B{row}'] = db_name
            ws_summary[f'C{row}'] = round(size_val, 2)
            if i == 1:
                ws_summary[f'D{row}'] = "En KÃ¼Ã§Ã¼k"
            else:
                diff_percent = ((size_val / disk_sizes[0][1]) - 1) * 100
                ws_summary[f'D{row}'] = f"+%{diff_percent:.1f}"
            
            for col in ['A', 'B', 'C', 'D']:
                ws_summary[f'{col}{row}'].border = border
                ws_summary[f'{col}{row}'].alignment = center_align
            row += 1
        
        # SÃ¼tun geniÅŸliklerini ayarla
        ws_summary.column_dimensions['A'].width = 12
        ws_summary.column_dimensions['B'].width = 25
        ws_summary.column_dimensions['C'].width = 15
        ws_summary.column_dimensions['D'].width = 15
        ws_summary.column_dimensions['E'].width = 15
        ws_summary.column_dimensions['F'].width = 15
        ws_summary.column_dimensions['G'].width = 15
        ws_summary.column_dimensions['H'].width = 15
        
        # DetaylÄ± SonuÃ§lar SayfasÄ±
        ws_details = wb.create_sheet("TÃ¼m Metrikler")
        
        row = 1
        ws_details['A1'] = "VeritabanÄ±"
        ws_details['B1'] = "Metrik"
        ws_details['C1'] = "DeÄŸer"
        ws_details['D1'] = "Birim"
        for col in ['A', 'B', 'C', 'D']:
            ws_details[f'{col}1'].fill = header_fill
            ws_details[f'{col}1'].font = header_font
            ws_details[f'{col}1'].border = border
            ws_details[f'{col}1'].alignment = center_align
        
        row = 2
        for db_name, metrics in self.results.items():
            for metric_name, metric_value in metrics.items():
                ws_details[f'A{row}'] = db_name
                ws_details[f'B{row}'] = metric_name
                
                if isinstance(metric_value, float):
                    ws_details[f'C{row}'] = round(metric_value, 4)
                else:
                    ws_details[f'C{row}'] = metric_value
                
                # Birim ekle
                if 'time' in metric_name.lower():
                    ws_details[f'D{row}'] = "saniye"
                elif 'size' in metric_name.lower() or 'mb' in metric_name.lower():
                    ws_details[f'D{row}'] = "MB"
                elif 'count' in metric_name.lower():
                    ws_details[f'D{row}'] = "adet"
                else:
                    ws_details[f'D{row}'] = "-"
                
                for col in ['A', 'B', 'C', 'D']:
                    ws_details[f'{col}{row}'].border = border
                    ws_details[f'{col}{row}'].alignment = center_align
                row += 1
        
        ws_details.column_dimensions['A'].width = 20
        ws_details.column_dimensions['B'].width = 30
        ws_details.column_dimensions['C'].width = 20
        ws_details.column_dimensions['D'].width = 15
        
        # Kaydet
        wb.save(output_file)
        print(f"\nğŸ’¾ DetaylÄ± sonuÃ§lar Excel dosyasÄ±na kaydedildi: {output_file}")
        print(f"   ğŸ“Š Ã–zet KarÅŸÄ±laÅŸtÄ±rma: Tablo formatÄ±nda karÅŸÄ±laÅŸtÄ±rmalÄ± sonuÃ§lar")
        print(f"   ğŸ“‹ TÃ¼m Metrikler: DetaylÄ± tÃ¼m Ã¶lÃ§Ã¼mler")
        
        # JSON'a da kaydet (yedek)
        json_file = output_file.replace('.xlsx', '.json')
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"   ğŸ’¾ JSON yedeÄŸi: {json_file}")

if __name__ == "__main__":
    benchmark = DatabaseBenchmark()
    benchmark.run_all_benchmarks()